{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "#idea\n",
    "#put everything in a try except to make it more robust? so it basically autorestarts itself\n",
    "#while True\n",
    "#  try\n",
    "    #code\n",
    "    #break\n",
    "    #excpet\n",
    "    #close windows and capture basically sending it into restart\n",
    "\n",
    "#We will use the coco dataset as it is very popular and gives us the ability to track some items that we are interested in\n",
    "#utilizing opencv version 4.5.1\n",
    "#cocodataset.org\n",
    "\n",
    "coco_names='coco.names'\n",
    "classes=[]\n",
    "with open(coco_names,'rt') as f: # read in all of the coco classes that we can detect from the file\n",
    "    for line in f:\n",
    "        currentPlace = line[:-1]\n",
    "        classes.append(currentPlace)\n",
    "\n",
    "configPath='ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt'\n",
    "weightsPath='frozen_inference_graph.pb'\n",
    "\n",
    "model =cv2.dnn_DetectionModel(weightsPath,configPath) #instantiate the detection object we will use and pass in the information it will need\n",
    "model.setInputSize(320,320) #set the size of the input width = height = 320\n",
    "model.setInputMean((127.5,127.5,127.5)) #sets the mean value for a frame which is subtracted from channels\n",
    "model.setInputSwapRB(True) #switch the red and blue color channels in the tensors\n",
    "model.setInputScale(1.0/127.5) #sets the scalefactor for a frame this is a scale multiplier for frames\n",
    "#values are standard values I saw in various examples\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3,640) #set the width of the capture\n",
    "cap.set(4,480) #set the height of the capture\n",
    "cap.set(10,80) #adjust brightness\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    classification, confidence, bbox = model.detect(frame, confThreshold=.45) #classify something in the capture if it is 65% confident or better\n",
    "    #get the classification of an object with what confidence and get the bounding box around it\n",
    "    #the bounding box will be the information that we use to make decisions\n",
    "    if len(classification)!=0:\n",
    "#     if True:\n",
    "        for obj, conf, bbox in zip(classification.flatten(),confidence.flatten(), bbox): #flatten compresses the matrix into a 1D based on contiguous row major order\n",
    "            #zip groups up coressponding elements in the 3 iterables of interest\n",
    "            cv2.rectangle(frame,bbox, color=(255,0,0), thickness=2) #draw a box around the objects that we want\n",
    "            cv2.putText(frame,classes[obj-1],(bbox[0]+10,bbox[1]+30), cv2.FONT_HERSHEY_PLAIN, 3, (0,255,0), thickness=3) #write the classification into the box\n",
    "            if classes[obj-1]==\"frisbee\" or classes[obj-1]==\"remote\" or classes[obj-1]==\"toothbrush\":\n",
    "#                 centerpoint=(int(bbox[0]+(bbox[2]-bbox[0])/2),int(bbox[1]+(bbox[3]-bbox[1])/2))\n",
    "                centerpoint=(int(bbox[0]+(bbox[2])/2),int(bbox[1]+(bbox[3])/2))\n",
    "                if centerpoint[0]<310:\n",
    "                    print('left')\n",
    "                if centerpoint[0]>330:\n",
    "                    print('right')\n",
    "                \n",
    "                cv2.circle(frame,centerpoint,10,(0,0,255),-1)\n",
    "    cv2.line(frame,(320,0),(320,480),(255,0,0),3) #draw the center line\n",
    "    #potentially implement a deadzone?\n",
    "    \n",
    "    cv2.imshow(\"tracking\", frame) #actually display the video\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): #quit if q is pressed MIGHT NEED TO CHANGE ON RAPSBERRY PI BECAUSE NOT A 64 BIT MACHINE\n",
    "        break\n",
    "cap.release() #release the capture\n",
    "cv2.destroyAllWindows() #destroy the capture window that was created\n",
    "\n",
    "\n",
    "#I learned about these OpenCV features primarily from the documentation and from Murtaza's Workshop video https://www.youtube.com/watch?v=HXDD7-EnGBY\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(cv2.dnn_DetectionModel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
